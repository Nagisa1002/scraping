{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0IdIjj/USTEl6FrewPmpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nagisa1002/scraping/blob/main/scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tczi2tm0Avw3"
      },
      "source": [
        "# リポジトリ名\n",
        "Yahoo Shopping Scraping\n",
        "\n",
        "## Dependency\n",
        "requirements\n",
        "\n",
        "## Usage\n",
        "1.   スクレイピングする際は#で囲まれた部分をチェック\n",
        "  *   キーワード(KEYWD)の変更\n",
        "  *抽出するページ数(PAGE)の変更\n",
        "  *   抽出する要素(ELE)の変更\n",
        "  *   ELEのタグ(ELE_TAG)の変更 -> 商品ページのHTMLをチェック\n",
        "  *   商品情報ページのタグ(DESC_LIST)の変更 ->リンク先のHTMLをチェック\n",
        "  *   get_elements関数の変更 -> 商品ページのHTMLをチェック\n",
        "2.   ランタイム→全てのセルを実行\n",
        "3.   ドライブのマウント(URLに飛んでパスワードを入力)\n",
        "\n",
        "\n",
        "## Attention\n",
        "* 商品情報を手に入れるにはその商品のリンクに飛ぶ必要があるが商品によって遷移先が異なるため，適宜DESC_LISTを更新すること．\n",
        "\n",
        "\n",
        "\n",
        "##References\n",
        "* CSSセレクタ : https://gammasoft.jp/support/css-selector-for-python-web-scraping/\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuvlH5X8E8Qg",
        "outputId": "87f994bc-aa45-422b-a843-1267280b60de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE7jYaqU0j6p"
      },
      "source": [
        "#requirements\n",
        "\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "import urllib.robotparser\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from datetime import datetime \n",
        "import re\n",
        "import sys\n",
        "from itertools import zip_longest\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlM1_WhM0pig"
      },
      "source": [
        "URL = 'https://shopping.yahoo.co.jp/'\n",
        "NAME='p' #yahoo's textbox name\n",
        "\n",
        "##############################################\n",
        "KEYWD = 'カーディガン' \n",
        "PAGE=2\n",
        "ELE = ['商品名', '値段', '画像', '商品説明', '商品URL']\n",
        "ELE_TAG = ['._2EW-04-9Eayr > span', '._3-CgJZLU91dR', '._2j-qvZxp4nZn', '._2Qs-G7hnS2-2'] #商品名，値段，画像，商品説明が含まれるHTMLタグ\n",
        "DESC_DICT = {'paypaymall':'.ItemDescription_text', 'bookoffonline':'body > div >table'}\n",
        "##############################################\n",
        "\n",
        "\n",
        "BASE_DIR = '/content/drive/My Drive/scraping'\n",
        "LOG_DIR = f'{BASE_DIR}/src'\n",
        "EXEC_TIME = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "SAVE_DIR=f'{LOG_DIR}/keyword_{EXEC_TIME}.csv'"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5uepIki0roF"
      },
      "source": [
        "def robot_check(url):\n",
        "    '''\n",
        "    confirm whether the scraping is permitted\n",
        "    '''\n",
        "\n",
        "    rp = urllib.robotparser.RobotFileParser()\n",
        "    rp.set_url(URL + 'robots.txt')\n",
        "    rp.read()\n",
        "\n",
        "    req_rate = rp.request_rate('*')\n",
        "    if req_rate is None:\n",
        "        print('クローリングへの指示書はありません')\n",
        "\n",
        "        req_URL = rp.can_fetch('*', URL)\n",
        "        if req_URL == True:\n",
        "            print('URLの取得も許可されています。これよりスクレイピングを開始します。')\n",
        "        else:\n",
        "            print('URLの取得が許可されていません。スクレイピングを中止します')\n",
        "    else:\n",
        "        print('クローリングへの指示書があります。問題がないか、利用規約を確認ください。')\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Y6v94I-MDW"
      },
      "source": [
        "def define_option():\n",
        "  '''\n",
        "  Chrome Startup Parameters\n",
        "  Chrome options : http://chrome.half-moon.org/43.html#l3e9a23d\n",
        "  '''\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument('--headless')\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  return options"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6TzvOv6-Ce"
      },
      "source": [
        "def get_elements(soup, ele_dict):\n",
        "    \"\"\"\n",
        "    pick up elements from html code\n",
        "    check the HTML code of target page and pull out attritude name you wants to get tags (class, id,...)\n",
        "\n",
        "    if you want's change or add elements, \n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    dict : {'商品名':[],....} \n",
        "\n",
        "    \"\"\"\n",
        "    tag = [soup.select(a) for a in ELE_TAG]\n",
        "    for t, p, i, d in zip_longest(*tag):\n",
        "      ##############################################\n",
        "      #title\n",
        "      if t!=None and t.text!='':\n",
        "        t=t.text\n",
        "        if 'クーポン有' in t :\n",
        "          t=re.split('[/／]', t)[1]\n",
        "        elif len(t)!=0 :\n",
        "          t=re.split('[/／]', t)[0]\n",
        "        ele_dict['商品名'].append(t)\n",
        "      \n",
        "      #price\n",
        "      if p!=None:\n",
        "        ele_dict['値段'].append(p.text)\n",
        "      \n",
        "      #img\n",
        "      if i!=None and i.get('src')!='https://s.yimg.jp/i/space.gif':\n",
        "        ele_dict['画像'].append(i.get('src'))\n",
        "      \n",
        "      #desc\n",
        "      if d!=None:\n",
        "        desc_url=d.get('href')\n",
        "        ele_dict['商品URL'].append(desc_url)\n",
        "        desc_html = requests.get(desc_url)\n",
        "        soup2 = BeautifulSoup(desc_html.content, 'lxml')\n",
        "        desc_text = soup2.select_one('.mdItemDescription')\n",
        "        if desc_text==None:\n",
        "          for i in DESC_DICT.keys():\n",
        "            if i in desc_url:\n",
        "              desc_text = soup2.select_one(DESC_DICT[i])\n",
        "              break\n",
        "\n",
        "        if desc_text!=None:\n",
        "          desc_text=desc_text.text\n",
        "        ele_dict['商品説明'].append(desc_text)\n",
        "\n",
        "     \n",
        "      ##############################################\n",
        "\n",
        "    n0 = len(ele_dict['商品名'])\n",
        "    print('number of emelemts :　', end='')\n",
        "    for i in ELE:\n",
        "      n = len(ele_dict[i])\n",
        "      if n == n0:\n",
        "        print('{0} length = {1}'.format(i, len(ele_dict[i])), end='')\n",
        "      else:\n",
        "        sys.exit('each list length is different. The number of all list lenght must be same. Check html code．')\n",
        "    print('')\n",
        "\n",
        "    return ele_dict"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIGF3sGuC4gE"
      },
      "source": [
        "def write_csv(ele_dict):\n",
        "  ele_list=[ele_dict[i] for i in ELE]\n",
        "  with open(SAVE_DIR, 'w', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(ELE)\n",
        "        for i in zip(*ele_list):\n",
        "          writer.writerow(i)\n",
        "  print('完了',SAVE_DIR)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL_EewY30yV5"
      },
      "source": [
        "def scraping():\n",
        "    ele_dict ={}\n",
        "    for  i in ELE:\n",
        "      ele_dict[i]=[]\n",
        "\n",
        "    option = define_option()\n",
        "\n",
        "    driver = webdriver.Chrome('chromedriver', options=option) \n",
        "    driver.implicitly_wait(10)\n",
        "    driver.get(URL)\n",
        "\n",
        "    # input keyword to textbox and go the result's page \n",
        "    elem_keyword = driver.find_element_by_name(NAME)\n",
        "    elem_keyword.send_keys(KEYWD)\n",
        "    driver.find_element_by_id('ss_srch_btn').click()\n",
        "    \n",
        "    # get url\n",
        "    cur_url = driver.current_url\n",
        "\n",
        "    for i in range(PAGE):\n",
        "      #get html\n",
        "      r_html = requests.get(cur_url)\n",
        "      soup = BeautifulSoup(r_html.content, 'lxml')\n",
        "\n",
        "      #go next page\n",
        "      cur_url=soup.select_one('.yHvnw7WEUQym > a')\n",
        "      cur_url=cur_url.get('href')\n",
        "\n",
        "      ele_dict = get_elements(soup, ele_dict)\n",
        "    write_csv(ele_dict)\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLQ55tco00IW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72699d80-2ad3-49a7-a0ff-c9afea573424"
      },
      "source": [
        "def main():\n",
        "    #robot_check(url) \n",
        "    scraping()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of emelemts :　商品名 length = 30値段 length = 30画像 length = 30商品説明 length = 30商品URL length = 30\n",
            "number of emelemts :　商品名 length = 60値段 length = 60画像 length = 60商品説明 length = 60商品URL length = 60\n",
            "完了 /content/drive/My Drive/scraping/src/keyword_20210607-082217.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}